This repository contains several data engineering projects demonstrating end-to-end ETL pipelines built using modern data stack technologies during my internship @Newwave Technologies.

The code demonstrates ETL processes built using Python/SQL to ingest, transform and load data using pySpark, Kafka, into Postgres database and further visualising data in Superset. 

The ETL jobs are scheduled and monitored using Apache Airflow running on a local Kubernetes cluster.

Technologies Used :

- PySpark
- Delta Lake
- Kafka
- Airflow
- Superset
- Kubernetes
- PostgreSQL
- LakeFS
- Great Expectations

